<div align="center">

# PolyMLP: A Polyglot Journey Through Multilayer Perceptrons

<!-- BADGES -->

![MIT License](https://img.shields.io/badge/license-MIT-blue.svg)
![Languages](https://img.shields.io/badge/languages-1-green)
![Topology](https://img.shields.io/badge/topology-6→16→16→1-orange)

<!-- BADGES -->

</div>

This challenge revolves around implementing minimal Multilayer Perceptrons (MLPs) in multiple programming languages using only the standard library.
It gives me a reason to explore new languages through a project I'm fairly comfortable with.
If you want to attempt the challenge yourself, feel free to open an issue, and I'll create a base branch! :)

---

<br>

<div align="center">

<!-- LEADERBOARD -->

<!-- LAST UPDATED 2025-10-04 17:18:59.971097 -->

Entry requirements: `MSE` < 0.10, `R²` > 0.90

Ranked by arithmetic mean of `Forward` and `Backward`

| Language                    | Forward (ms)      | Backward (ms)     | Epoch (s)         | MAE    | MSE    | R²     |
|-----------------------------|-------------------|-------------------|-------------------|--------|--------|--------|
| [Python](JOURNAL.md#Python) | 0.748312 ± 0.0368 | 1.112331 ± 0.0576 | 0.493498 ± 0.0167 | 0.1871 | 0.0561 | 0.9393 |

<!-- LEADERBOARD -->

</div>

<br>

---

## Overview

### Data

To train and benchmark each MLP, a [Friedman function](data/friedman.ipynb) is used.
The [data/](data/) directory includes a generated [train.csv](data/train.csv) and [test.csv](data/test.csv).

### Implementations

Each language's code resides in a subdirectory in [implementations/](implementations/) and contains a brief README.md on usage.
Note that the implementations aren't documented beyond this README.md, as I mainly want to explore new languages rather than focus on writing docstrings.
However, if you would like to attempt this challenge yourself and need some help, feel free to open an issue, and I'll draft a cheat sheet.

### Journal

You can find chronological implementation notes, reflections, and language-specific insights in [JOURNAL.md](JOURNAL.md).
Alternatively, you can click the links in the leaderboard, and it will take you right to the respective entry.

### Leaderboard

The leaderboard includes MAE, MSE, and R² for transparency, but these metrics are not used to rank implementations.
In fact, they are not comparable due to differing seeding implementations and the [adaptability of the learning rate](#rules).
To update the leaderboard, create and activate the [Anaconda environment](#requirements) and run `python main.py`.
Note that the `results/` subdirectories are not included in the repository and can be generated by running an implementation.

Take the benchmarks with a grain of salt, as I may choose whether to prioritize runtime performance in an implementation.
This is purely a byproduct of me needing a reason to explore new languages.
I'll run all code on my local machine while ensuring minimal system interference.

### PolyMLP

The [polymlp](polymlp/) module is a simple parser that inserts the leaderboard into this README.md.
If you want to use it in your own project, you can find a high-level docstring in the function invoked in [main.py](main.py), read the [subsection on writing results](#results), and set up the provided [Anaconda environment](#requirements).
It should be fairly easy to adapt the [polymlp](polymlp/) module to tasks beyond 1D regression.

---

## Rules

The reasoning behind many of the rules is to keep the language performance as comparable as possible, while not constraining myself too much.
For instance, the implementation of matrix multiplication and the matrix storage format are left unconstrained to allow some competition with languages whose standard libraries include BLAS-level performance.
Similarly, the learning rate is left unconstrained to guarantee the possibility of fulfilling the entry requirements.
However, I'll try to use a learning rate of `5e-3` where possible and note deviations from this value in [JOURNAL.md](JOURNAL.md).
Please open an issue if you notice any rule violations (e.g., accidental multithreading).

### Implementation

- Use only the standard library.
- Use a single thread.
- Use default compiler settings.
- No warm-up runs in JIT-compiled languages.
- Seed the process with `42` at the top of the entry point.
- Use `64-bit` floating point precision for every operation.
- Use ReLU activation units only.
- Compute gradients manually.
- Use the topology `6, 16, 16, 1`.

### Training

- Use the provided [train.csv](data/train.csv).
- Initialize weights from [U(-√k, √k), where k = 1 / fan_in](https://docs.pytorch.org/docs/stable/generated/torch.nn.Linear.html).
- Initialize biases to zero.
- Optimize the MSE using minibatch SGD.
- Set the batch size to `32`.
This divides the training samples evenly.
- Shuffle the batch indices each epoch.
- Train for `100` epochs.

### Testing

- Use the provided [test.csv](data/test.csv).
- Compute test predictions without batching to avoid bugs.

### Results

All values must be stored using `64-bit` floating point precision.

| File                           | Columns               | Notes                                   |
|--------------------------------|-----------------------|-----------------------------------------|
| `results/batch_times.csv`      | `forward`, `backward` | Recorded **in seconds** during training |
| `results/epoch_times.csv`      | `epoch`               | Recorded **in seconds** during training |
| `results/test_predictions.csv` | `prediction`          | **Ordered** predictions on test data    |

---

## Requirements

To use [friedman.ipynb](data/friedman.ipynb) or the [polymlp](polymlp/) module you can create and activate an Anaconda environment by running

```bash
conda env create -f environment.yml
conda activate polymlp
```

---

## Licensing

This project is [MIT-licensed](LICENSE).
